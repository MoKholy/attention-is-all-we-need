# attention-is-all-we-need
Yet another implementation of the Attention is All You Need paper: https://arxiv.org/abs/1706.03762 in pytorch. 


## WIP
    - [x] Build Transformer architecture
    - [ ] Add testcases to check architecture works
    - [ ] Add label smoothing
    - [ ] Add different optimizers and lr schedulers
    - [ ] Build Training pipeline
    - [ ] Add weights & biases metrics tracking
    - [ ] Train on some dataset ( current ideas include franco to arabic)

