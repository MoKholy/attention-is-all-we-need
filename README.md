# attention-is-all-we-need
Yet another implementation of the Attention is All You Need paper: https://arxiv.org/abs/1706.03762 in pytorch. 


## WIP
- [x] Build Transformer architecture
- [x] Add testcases to check architecture works
- [ ] Correct dropout and norm layers
- [ ] Add dataset class and data pipeline
- [ ] Add different optimizers and lr schedulers
- [ ] Build Training pipeline
- [ ] Add label smoothing
- [ ] Add weights & biases metrics tracking
- [ ] Train on some dataset ( current ideas include franco to arabic)

